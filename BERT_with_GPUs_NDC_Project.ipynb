{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT with GPUs - NDC Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rinsworth/NDC-Project/blob/master/BERT_with_GPUs_NDC_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48INq3A3bLtC"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlPz6DAsrybX"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHh1Zlw8BEJG",
        "outputId": "c69a17ba-ef98-4a8a-d0e6-207dcb3827d0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT8eLU8Gc9lb",
        "outputId": "917cd65a-343f-45f7-b72d-b04f4c268cb0"
      },
      "source": [
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.7/dist-packages (0.10.28)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUjpOXFAD7Dg"
      },
      "source": [
        "## Specify BERT version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-H5cOjgOzeD"
      },
      "source": [
        "bert_version = 'bert-base-uncased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-8cgzac14W"
      },
      "source": [
        "## W&B Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMxkDZCHdH0m",
        "outputId": "95b0eed0-f095-4fd6-e324-5daf5ac56887"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merxiao\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI-Odm_BLa1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54097cfb-c832-4b2b-d659-ace8b3b6c0bd"
      },
      "source": [
        "%env WANDB_PROJECT = ndc-project\n",
        "%env WANDB_LOG_MODEL = true "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: WANDB_PROJECT=ndc-project\n",
            "env: WANDB_LOG_MODEL=true\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qZW57OQTp7S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "5da5f3a9-12ae-4459-f6c0-e878d751f082"
      },
      "source": [
        "lr = 9.0625e-5\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "warmup_steps = 500\n",
        "wandb_config = {\n",
        "    'learning_rate': lr,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'warmup_steps': warmup_steps\n",
        "}\n",
        "wandb.init(project='ndc-project', config=wandb_config, name=f'{lr}-lr_{batch_size}-batch_{epochs}-epochs_{warmup_steps}-warmups')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.28<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">9.0625e-05-lr_8-batch_10-epochs_500-warmups</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/erxiao/ndc-project\" target=\"_blank\">https://wandb.ai/erxiao/ndc-project</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/erxiao/ndc-project/runs/2i39tzsz\" target=\"_blank\">https://wandb.ai/erxiao/ndc-project/runs/2i39tzsz</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210503_081711-2i39tzsz</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fc958d63f10>"
            ],
            "text/html": [
              "<h1>Run(2i39tzsz)</h1><iframe src=\"https://wandb.ai/erxiao/ndc-project/runs/2i39tzsz\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfObI0Lcr51w"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayrUKIjyCQGH"
      },
      "source": [
        "def good_update_interval(total_iters, num_desired_updates):\n",
        "    '''\n",
        "    This function will try to pick an intelligent progress update interval \n",
        "    based on the magnitude of the total iterations.\n",
        "\n",
        "    Parameters:\n",
        "      `total_iters` - The number of iterations in the for-loop.\n",
        "      `num_desired_updates` - How many times we want to see an update over the \n",
        "                              course of the for-loop.\n",
        "    '''\n",
        "    # Divide the total iterations by the desired number of updates. Most likely\n",
        "    # this will be some ugly number.\n",
        "    exact_interval = total_iters / num_desired_updates\n",
        "\n",
        "    # The `round` function has the ability to round down a number to, e.g., the\n",
        "    # nearest thousandth: round(exact_interval, -3)\n",
        "    #\n",
        "    # To determine the magnitude to round to, find the magnitude of the total,\n",
        "    # and then go one magnitude below that.\n",
        "\n",
        "    # Get the order of magnitude of the total.\n",
        "    order_of_mag = len(str(total_iters)) - 1\n",
        "\n",
        "    # Our update interval should be rounded to an order of magnitude smaller. \n",
        "    round_mag = order_of_mag - 1\n",
        "\n",
        "    # Round down and cast to an int.\n",
        "    update_interval = int(round(exact_interval, -round_mag))\n",
        "\n",
        "    # Don't allow the interval to be zero!\n",
        "    if update_interval == 0:\n",
        "        update_interval = 1\n",
        "\n",
        "    return update_interval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5LVFY7MPqRf"
      },
      "source": [
        "def make_smart_batches(text_samples, labels, batch_size):\n",
        "    '''\n",
        "    This function combines all of the required steps to prepare batches.\n",
        "    '''\n",
        "\n",
        "    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n",
        "\n",
        "    # =========================\n",
        "    #   Tokenize & Truncate\n",
        "    # =========================\n",
        "\n",
        "    full_input_ids = []\n",
        "\n",
        "    # Tokenize all training examples\n",
        "    print('Tokenizing {:,} samples...'.format(len(labels)))\n",
        "\n",
        "    # Choose an interval on which to print progress updates.\n",
        "    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n",
        "\n",
        "    # For each training example...\n",
        "    for text in text_samples:\n",
        "        \n",
        "        # Report progress.\n",
        "        if ((len(full_input_ids) % update_interval) == 0):\n",
        "            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
        "\n",
        "        max_len = 400\n",
        "\n",
        "        # Tokenize the sample.\n",
        "        input_ids = tokenizer.encode(text=text,              # Text to encode.\n",
        "                                    add_special_tokens=True, # Do add specials.\n",
        "                                    max_length=max_len,      # Do Truncate!\n",
        "                                    truncation=True,         # Do Truncate!\n",
        "                                    padding=False)           # DO NOT pad.\n",
        "                                    \n",
        "        # Add the tokenized result to our list.\n",
        "        full_input_ids.append(input_ids)\n",
        "        \n",
        "    print('DONE.')\n",
        "    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n",
        "\n",
        "    # =========================\n",
        "    #      Select Batches\n",
        "    # =========================    \n",
        "\n",
        "    # Sort the two lists together by the length of the input sequence.\n",
        "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
        "\n",
        "    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n",
        "\n",
        "    import random\n",
        "\n",
        "    # List of batches that we'll construct.\n",
        "    batch_ordered_sentences = []\n",
        "    batch_ordered_labels = []\n",
        "\n",
        "    print('Creating batches of size {:}...'.format(batch_size))\n",
        "\n",
        "    # Choose an interval on which to print progress updates.\n",
        "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
        "    \n",
        "    # Loop over all of the input samples...    \n",
        "    while len(samples) > 0:\n",
        "        \n",
        "        # Report progress.\n",
        "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
        "            and not len(batch_ordered_sentences) == 0):\n",
        "            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "        # `to_take` is our actual batch size. It will be `batch_size` until \n",
        "        # we get to the last batch, which may be smaller. \n",
        "        to_take = min(batch_size, len(samples))\n",
        "\n",
        "        # Pick a random index in the list of remaining samples to start\n",
        "        # our batch at.\n",
        "        select = random.randint(0, len(samples) - to_take)\n",
        "\n",
        "        # Select a contiguous batch of samples starting at `select`.\n",
        "        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n",
        "        batch = samples[select:(select + to_take)]\n",
        "\n",
        "        #print(\"Batch length:\", len(batch))\n",
        "\n",
        "        # Each sample is a tuple--split them apart to create a separate list of \n",
        "        # sequences and a list of labels for this batch.\n",
        "        batch_ordered_sentences.append([s[0] for s in batch])\n",
        "        batch_ordered_labels.append([s[1] for s in batch])\n",
        "\n",
        "        # Remove these samples from the list.\n",
        "        del samples[select:select + to_take]\n",
        "\n",
        "    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "    # =========================\n",
        "    #        Add Padding\n",
        "    # =========================    \n",
        "\n",
        "    print('Padding out sequences within each batch...')\n",
        "\n",
        "    py_inputs = []\n",
        "    py_attn_masks = []\n",
        "    py_labels = []\n",
        "\n",
        "    # For each batch...\n",
        "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
        "\n",
        "        # New version of the batch, this time with padded sequences and now with\n",
        "        # attention masks defined.\n",
        "        batch_padded_inputs = []\n",
        "        batch_attn_masks = []\n",
        "        \n",
        "        # First, find the longest sample in the batch. \n",
        "        # Note that the sequences do currently include the special tokens!\n",
        "        max_size = max([len(sen) for sen in batch_inputs])\n",
        "\n",
        "        # For each input in this batch...\n",
        "        for sen in batch_inputs:\n",
        "            \n",
        "            # How many pad tokens do we need to add?\n",
        "            num_pads = max_size - len(sen)\n",
        "\n",
        "            # Add `num_pads` padding tokens to the end of the sequence.\n",
        "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
        "\n",
        "            # Define the attention mask--it's just a `1` for every real token\n",
        "            # and a `0` for every padding token.\n",
        "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
        "\n",
        "            # Add the padded results to the batch.\n",
        "            batch_padded_inputs.append(padded_input)\n",
        "            batch_attn_masks.append(attn_mask)\n",
        "\n",
        "        # Our batch has been padded, so we need to save this updated batch.\n",
        "        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
        "        # Todo - Michael's code specified \"dtype=torch.long\"\n",
        "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
        "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
        "        py_labels.append(torch.tensor(batch_labels))\n",
        "    \n",
        "    print('  DONE.')\n",
        "\n",
        "    # Return the smart-batched dataset!\n",
        "    return (py_inputs, py_attn_masks, py_labels, batch_ordered_sentences, batch_ordered_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7MKfS1kUS2T"
      },
      "source": [
        "# Data processing\n",
        "(**MAKE SURE** to go on the left bar, click on 'Files', and upload the training data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaitfMMj2-0c"
      },
      "source": [
        "![Screen Shot 2021-03-22 at 2.34.31 PM.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXRh21i4bUJj"
      },
      "source": [
        "## Read data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k6rIUqMDTPY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '20210302_15_NDC_Labeled_Sentences_10_Words.csv'\n",
        "\n",
        "def load_data(data_dir):\n",
        "  df = pd.read_csv(data_dir)\n",
        "  df.drop(df[df['label'] == '_no_label'].index, inplace=True)   # drop unlabeled sentences\n",
        "\n",
        "  # print('Labels:', np.unique(df['label']))\n",
        "  # print('Countries:', np.unique(df['iso']))\n",
        "  # print('# of countries:', len(np.unique(df['iso'])))\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdTF0iTMbaUk"
      },
      "source": [
        "## Split into training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12yO0ALUH7yb"
      },
      "source": [
        "# Divide set of countries into mutually exclusive train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# mode to split by country or random\n",
        "def split_train_test(df):\n",
        "  # Split data into list of countries\n",
        "  df_by_iso = list(df.groupby('iso'))\n",
        "  train, test = train_test_split(df_by_iso, train_size=0.80)\n",
        "\n",
        "  test, validation = train_test_split(test, test_size=0.50)\n",
        "\n",
        "  # Print country statistics\n",
        "  print(f'Training dataset contains {len(train)} countries, test dataset contains {len(test)} countries, and validation dataset contains {len(validation)} countries, for a total of {len(train) + len(test) + len(validation)} countries.')\n",
        "\n",
        "  # Merge countries back into train and test datasets.\n",
        "  # Each element of train/test is a tuple (iso, data), \n",
        "  # so grab data and respective column for each country.\n",
        "  train_text        = np.concatenate([np.array(train[i][1]['sentence'])      for i in range(len(train))])\n",
        "  train_labels      = np.concatenate([np.array(train[i][1]['label'])         for i in range(len(train))])\n",
        "  test_text         = np.concatenate([np.array(test[i][1]['sentence'])       for i in range(len(test))])\n",
        "  test_labels       = np.concatenate([np.array(test[i][1]['label'])          for i in range(len(test))])\n",
        "  validation_text   = np.concatenate([np.array(validation[i][1]['sentence']) for i in range(len(validation))])\n",
        "  validation_labels = np.concatenate([np.array(validation[i][1]['label'])    for i in range(len(validation))])\n",
        "\n",
        "  # Sanity check\n",
        "  assert len(train_text)      == len(train_labels)\n",
        "  assert len(test_text)       == len(test_labels)\n",
        "  assert len(validation_text) == len(validation_labels)\n",
        "\n",
        "  # Print sentence statistics\n",
        "  print(f'Training dataset contains {len(train_text)} sentences, test dataset contains {len(test_text)} sentences, and validation dataset contains {len(validation_text)} sentences, for a total of {len(train_text) + len(test_text) + len(validation_text)} sentences.')\n",
        "  \n",
        "  return (train_text, train_labels, test_text, test_labels, validation_text, validation_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mYZ0ljXUgSQ"
      },
      "source": [
        "## Class balance visualization (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1deAxyeeuOQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "\n",
        "def see_train_dist(train_labels):\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.25)\n",
        "  plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "  # Plot the number of tokens of each length.\n",
        "  ax = sns.countplot(train_labels)\n",
        "  # TODO: ^ same for test labels.\n",
        "\n",
        "  # Add labels\n",
        "  plt.title('Class Distribution')\n",
        "  plt.xlabel('Category')\n",
        "  plt.ylabel('# of Training Samples')\n",
        "\n",
        "  # Add thousands separators to the y-axis labels.\n",
        "  ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hlIEpiAh6M3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def see_test_dist(test_labels):\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.25)\n",
        "  plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "\n",
        "  # Plot the number of tokens of each length.\n",
        "  ax = sns.countplot(test_labels)\n",
        "  # TODO: ^ same for test labels.\n",
        "\n",
        "  # Add labels\n",
        "  plt.title('Class Distribution')\n",
        "  plt.xlabel('Category')\n",
        "  plt.ylabel('# of Validation Samples')\n",
        "\n",
        "  # Add thousands separators to the y-axis labels.\n",
        "  import matplotlib as mpl\n",
        "  ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMg50tVwfahi"
      },
      "source": [
        "## Encode labels from string to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0QTylSVfaBU"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_labels(train_labels, val_labels, test_labels):\n",
        "  encoder = LabelEncoder()\n",
        "  encoder.fit(df['label'])\n",
        "\n",
        "  train_labels = encoder.transform(train_labels)\n",
        "  val_labels = encoder.transform(val_labels)\n",
        "  test_labels = encoder.transform(test_labels)\n",
        "\n",
        "  return (train_labels, val_labels, test_labels, encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g64fx1_Rrq8J"
      },
      "source": [
        "# Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTyVBQFtVOUi"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxcYhHBBrfq7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "# print('Loading BERT tokenizer...')\n",
        "# tokenizer = BertTokenizer.from_pretrained(bert_version, do_lower_case=True)\n",
        "# NOTE: moved to tokenize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQkxcEeZDzx"
      },
      "source": [
        "## Tokenize (no padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iehCtObYiJf"
      },
      "source": [
        "def tokenize(train_text, bert_version):\n",
        "  # Load the BERT tokenizer.\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained(bert_version, do_lower_case=True)\n",
        "\n",
        "  max_len = 400\n",
        "\n",
        "  full_input_ids = []\n",
        "  labels = []\n",
        "\n",
        "  # Tokenize all training examples\n",
        "  print('Tokenizing {:,} training samples...'.format(len(train_text)))\n",
        "\n",
        "  # Choose an interval on which to print progress updates.\n",
        "  update_interval = good_update_interval(total_iters=len(train_text), num_desired_updates=10)\n",
        "\n",
        "  # For each training example...\n",
        "  for text in train_text:\n",
        "      \n",
        "      # Report progress.\n",
        "      if ((len(full_input_ids) % update_interval) == 0):\n",
        "          print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n",
        "\n",
        "      # Tokenize the sentence.\n",
        "      input_ids = tokenizer.encode(text=text,           # Movie review text\n",
        "                                  add_special_tokens=True, # Do add specials.\n",
        "                                  max_length=max_len,  # Do truncate to `max_len`\n",
        "                                  truncation=True,     # Do truncate!\n",
        "                                  padding=False)       # Don't pad!\n",
        "                                  \n",
        "      # Add the tokenized result to our list.\n",
        "      full_input_ids.append(input_ids)\n",
        "      \n",
        "  print('DONE.')\n",
        "  print('{:>10,} samples'.format(len(full_input_ids)))\n",
        "\n",
        "  return (full_input_ids, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffhsvOoCZGhc"
      },
      "source": [
        "## Unsorted data visualization (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnLslr5VZQuH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def see_unsorted_data(full_input_ids):\n",
        "  # Get all of the lengths.\n",
        "  unsorted_lengths = [len(x) for x in full_input_ids]\n",
        "\n",
        "  # Use plot styling from seaborn.\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "  plt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")\n",
        "\n",
        "  plt.xlabel('Sample Number')\n",
        "  plt.ylabel('Sequence Length')\n",
        "  plt.title('Samples BEFORE Sorting')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPREDUxHaI40"
      },
      "source": [
        "## Sort data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBesjNYe_pVB"
      },
      "source": [
        "def sort_data(full_input_ids, train_labels):\n",
        "  # Sort the two lists together by the length of the input sequence.\n",
        "  train_samples = sorted(zip(full_input_ids, train_labels), key=lambda x: len(x[0]))\n",
        "  train_samples[0]\n",
        "  print('Shortest sample:', len(train_samples[0][0]))\n",
        "  print('Longest sample:', len(train_samples[-1][0]))\n",
        "\n",
        "  return train_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwuwisoREQ0U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WThxaPL6ZvH7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0kMJ2RkaN6g"
      },
      "source": [
        "## Sorted data visualization (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWvR18VsbmK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def see_sorted_data(train_samples):\n",
        "  # Get the new list of lengths after sorting.\n",
        "  sorted_lengths = [len(s[0]) for s in train_samples]\n",
        "\n",
        "  # Use plot styling from seaborn.\n",
        "  sns.set(style='darkgrid')\n",
        "\n",
        "  # Increase the plot size and font size.\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "  plt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n",
        "\n",
        "  plt.xlabel('Sample Number')\n",
        "  plt.ylabel('Sequence Length')\n",
        "  plt.title('Samples after Sorting')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5AFgNIqb_66"
      },
      "source": [
        "## Random batch selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKHyRG5cLgk"
      },
      "source": [
        "### Choose batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFU-pkIkcPec"
      },
      "source": [
        "# batch_size = wandb.config.batch_size\n",
        "# NOTE: moved to select_batches()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh8y-6jDcSZz"
      },
      "source": [
        "### Selection algorithm information\n",
        "\n",
        "Rather than dividing the batches up in order, we will still add a degree of **randomness** to our selection.\n",
        "\n",
        "Here's the process:\n",
        "\n",
        "1.   Pick a random starting point in the (sorted) list of samples.\n",
        "2.   Grab a contiguous batch of samples starting from that point.\n",
        "3.   Delete those samples from the list, and repeat until all of the samples have been grabbed.\n",
        "\n",
        "This will result in some **fragmentation** of the list, which means it won't be quite as efficient as if we just sliced up the batches in sorted order.\n",
        "\n",
        "The benefit is that our path through the training set can still have a degree of randomness. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXUsk9ccdrb-"
      },
      "source": [
        "### Perform batch selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b53M03bdxiF"
      },
      "source": [
        "import random\n",
        "\n",
        "def select_batches(train_samples):\n",
        "  batch_size = wandb.config.batch_size\n",
        "  # List of batches that we'll construct.\n",
        "  batch_ordered_sentences = []\n",
        "  batch_ordered_labels = []\n",
        "\n",
        "  print('Creating training batches of size {:}'.format(batch_size))\n",
        "\n",
        "  # Loop over all of the input samples...    \n",
        "  while len(train_samples) > 0:\n",
        "      \n",
        "      # Report progress.\n",
        "      if ((len(batch_ordered_sentences) % 500) == 0):\n",
        "          print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "      # `to_take` is our actual batch size. It will be `batch_size` until \n",
        "      # we get to the last batch, which may be smaller. \n",
        "      to_take = min(batch_size, len(train_samples))\n",
        "\n",
        "      # Pick a random index in the list of remaining samples to start\n",
        "      # our batch at.\n",
        "      select = random.randint(0, len(train_samples) - to_take)\n",
        "\n",
        "      # Select a contiguous batch of samples starting at `select`.\n",
        "      batch = train_samples[select:(select + to_take)]\n",
        "\n",
        "      # Each sample is a tuple--split them apart to create a separate list of \n",
        "      # sequences and a list of labels for this batch.\n",
        "      batch_ordered_sentences.append([s[0] for s in batch])\n",
        "      batch_ordered_labels.append([s[1] for s in batch])\n",
        "\n",
        "      # Remove these samples from the list.\n",
        "      del train_samples[select:select + to_take]\n",
        "\n",
        "  print('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))\n",
        "\n",
        "  return (batch_size, batch_ordered_sentences, batch_ordered_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqjvubeeI_5"
      },
      "source": [
        "## Add padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI72yHq3eL0y"
      },
      "source": [
        "import torch\n",
        "\n",
        "def add_padding(batch_ordered_sentences, batch_ordered_labels, tokenizer):\n",
        "  py_inputs = []\n",
        "  py_attn_masks = []\n",
        "  py_labels = []\n",
        "\n",
        "  # For each batch...\n",
        "  for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
        "\n",
        "      # New version of the batch, this time with padded sequences and now with\n",
        "      # attention masks defined.\n",
        "      batch_padded_inputs = []\n",
        "      batch_attn_masks = []\n",
        "      \n",
        "      # First, find the longest sample in the batch. \n",
        "      # Note that the sequences do currently include the special tokens!\n",
        "      max_size = max([len(sen) for sen in batch_inputs])\n",
        "\n",
        "      #print('Max size:', max_size)\n",
        "\n",
        "      # For each input in this batch...\n",
        "      for sen in batch_inputs:\n",
        "          \n",
        "          # How many pad tokens do we need to add?\n",
        "          num_pads = max_size - len(sen)\n",
        "\n",
        "          # Add `num_pads` padding tokens to the end of the sequence.\n",
        "          padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
        "\n",
        "          # Define the attention mask--it's just a `1` for every real token\n",
        "          # and a `0` for every padding token.\n",
        "          attn_mask = [1] * len(sen) + [0] * num_pads\n",
        "\n",
        "          # Add the padded results to the batch.\n",
        "          batch_padded_inputs.append(padded_input)\n",
        "          batch_attn_masks.append(attn_mask)\n",
        "\n",
        "      # Our batch has been padded, so we need to save this updated batch.\n",
        "      # We also need the inputs to be PyTorch tensors, so we'll do that here.\n",
        "      py_inputs.append(torch.tensor(batch_padded_inputs))\n",
        "      py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
        "      py_labels.append(torch.tensor(batch_labels))\n",
        "\n",
        "  return (py_inputs, py_attn_masks, py_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3ZsQo4DkXQJ"
      },
      "source": [
        "## Padding visualization (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPQgl1wYLa59"
      },
      "source": [
        "def see_padding(py_inputs, train_text):\n",
        "  # Get the new list of lengths after sorting.\n",
        "  padded_lengths = []\n",
        "\n",
        "  # For each batch...\n",
        "  for batch in py_inputs:\n",
        "      \n",
        "      # For each sample...\n",
        "      for s in batch:\n",
        "      \n",
        "          # Record its length.\n",
        "          padded_lengths.append(len(s))\n",
        "\n",
        "  # Sum up the lengths to the get the total number of tokens after smart batching.\n",
        "  smart_token_count = np.sum(padded_lengths)\n",
        "\n",
        "  # To get the total number of tokens in the dataset using fixed padding, it's\n",
        "  # as simple as the number of samples times our `max_len` parameter (that we\n",
        "  # would pad everything to).\n",
        "  fixed_token_count = len(train_text) * max_len\n",
        "\n",
        "  # Calculate the percentage reduction.\n",
        "  prcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n",
        "\n",
        "  print('Total tokens:')\n",
        "  print('   Fixed Padding: {:,}'.format(fixed_token_count))\n",
        "  print('  Smart Batching: {:,}  ({:.1%} less)'.format(smart_token_count, prcnt_reduced))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmm8jGA1kSmY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moGP-9ufmcNy"
      },
      "source": [
        "## Load pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaNwO406ko6J"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "def load_model(bert_version):\n",
        "  config = AutoConfig.from_pretrained(pretrained_model_name_or_path=bert_version, num_labels=11)\n",
        "\n",
        "  print('Config type:', str(type(config)), '\\n')\n",
        "  print(config)\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=bert_version, config=config)\n",
        "\n",
        "  print('\\nModel type:', str(type(model)))\n",
        "\n",
        "  print('\\nLoading model to GPU...')\n",
        "  device = torch.device('cuda')\n",
        "  print('   GPU:', torch.cuda.get_device_name(0))\n",
        "  desc = model.to(device)\n",
        "  print('   DONE.')\n",
        "\n",
        "  return (model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssE7yOo2mihj"
      },
      "source": [
        "## Optimizer and Learning Rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JhA43zuneNo"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def get_optimizer_scheduler(model, py_inputs):\n",
        "  optimizer = AdamW(model.parameters(), lr=wandb.config.learning_rate, eps=1e-8)\n",
        "  \n",
        "  epochs = wandb.config.epochs\n",
        "\n",
        "  total_steps = len(py_inputs) * epochs\n",
        "\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=wandb.config.warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "  return (optimizer, scheduler, epochs)\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_wHJ4oQnzXY"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B-LzLX_n1a4"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def train_loop(model, py_inputs, py_attn_masks, py_labels, epochs, train_text, train_labels, batch_size, device, optimizer, scheduler, encoder):\n",
        "  # randomize and record\n",
        "  seed_val = 321\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  np.random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "  torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "  training_stats = []\n",
        "\n",
        "  update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
        "\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  global_step = 0\n",
        "  num_val_inc = 0\n",
        "  prev_val_acc = 0\n",
        "  max_val_acc = 0\n",
        "  VAL_STOP = 3\n",
        "\n",
        "  wandb.log({'learning_rate':get_learning_rate(optimizer)}, step=global_step)\n",
        "  wandb.log({'training accuracy': 1/11}, step=global_step)\n",
        "  wandb.log({'validation accuracy': 1/11}, step=global_step)\n",
        "\n",
        "  for epoch_i in range(epochs):\n",
        "    print(\"\")\n",
        "    print(f\"{'='*8} Epoch {epoch_i+1} / {epochs} {'='*8}\")\n",
        "\n",
        "    if epoch_i > 0:\n",
        "      (py_inputs, py_attn_masks, py_labels, _, _) = make_smart_batches(train_text, train_labels, batch_size)\n",
        "    \n",
        "    print(f\"Training on {len(py_inputs)} batches...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step in range(len(py_inputs)):\n",
        "      global_step += 1\n",
        "\n",
        "      if step % update_interval == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "\n",
        "        steps_per_sec = (time.time() - t0) / step\n",
        "        remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
        "        remaining = format_time(remaining_sec)\n",
        "\n",
        "        print(f\"    Batch {step} of {len(py_inputs)}.   Elapsed: {elapsed}.   Remaining: {remaining}\")\n",
        "\n",
        "      b_input_ids = py_inputs[step].to(device)\n",
        "      b_input_mask = py_attn_masks[step].to(device)\n",
        "      b_labels = py_labels[step].to(device)\n",
        "\n",
        "      model.zero_grad()\n",
        "      \n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "      # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      wandb.log({'train_batch_loss':outputs.loss.item()}, step=global_step)\n",
        "\n",
        "      total_train_loss += outputs.loss\n",
        "\n",
        "      outputs.loss.backward()\n",
        "\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      wandb.log({'learning_rate':get_learning_rate(optimizer)}, step=global_step)\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(py_inputs)\n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    # eval loop\n",
        "    train_preds, train_true_labels, _, _, _ = eval_model(model, train_text, train_labels, batch_size, device)\n",
        "    val_preds, val_true_labels, _, _, val_loss = eval_model(model, validation_text, validation_labels, batch_size, device)\n",
        "\n",
        "    # print val accuracy\n",
        "    print_accuracy(train_preds, train_true_labels, encoder, step=global_step, title='training')\n",
        "    val_acc = print_accuracy(val_preds, val_true_labels, encoder, step=global_step, title='validation')\n",
        "\n",
        "    # save best model\n",
        "    if val_acc > max_val_acc:\n",
        "      max_val_acc = val_acc\n",
        "      torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "    wandb.log({'train_epoch_loss':avg_train_loss}, step=global_step)\n",
        "    wandb.log({'validation_epoch_loss':val_loss}, step=global_step)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"   Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"   Average validation loss: {0:.2f}\".format(val_loss))\n",
        "    print(f\"   Training epoch took: {training_time}\")\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Loss': val_loss\n",
        "        }\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSYwoDA3LiQY"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUZWA9FMLrzF"
      },
      "source": [
        "## Smart-batch testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGuZAHn_S-fG"
      },
      "source": [
        "# Use our new function to completely prepare our dataset.\n",
        "# (py_inputs, py_attn_masks, py_labels) = make_smart_batches(test_text, test_labels, batch_size)\n",
        "# NOTE: moved to eval_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7W5pmqAL-k7"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "source": [
        "def eval_model(model, test_text, test_labels, batch_size, device):\n",
        "  # Prediction on test set\n",
        "  # Refactor to eval_loop(model, data)\n",
        "\n",
        "  (py_inputs, py_attn_masks, py_labels, batch_ordered_sentences, batch_ordered_labels) = make_smart_batches(test_text, test_labels, batch_size)\n",
        "\n",
        "  print('Predicting labels for {:,} test sentences...'.format(len(test_labels)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Choose an interval on which to print progress updates.\n",
        "  update_interval = good_update_interval(total_iters=len(py_inputs), num_desired_updates=10)\n",
        "\n",
        "  # Measure elapsed time.\n",
        "  t0 = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  # For each batch of training data...\n",
        "  for step in range(0, len(py_inputs)):\n",
        "\n",
        "      # Progress update every 100 batches.\n",
        "      if step % update_interval == 0 and not step == 0:\n",
        "          # Calculate elapsed time in minutes.\n",
        "          elapsed = format_time(time.time() - t0)\n",
        "          \n",
        "          # Calculate the time remaining based on our progress.\n",
        "          steps_per_sec = (time.time() - t0) / step\n",
        "          remaining_sec = steps_per_sec * (len(py_inputs) - step)\n",
        "          remaining = format_time(remaining_sec)\n",
        "\n",
        "          # Report progress.\n",
        "          print('  Batch {:>7,}  of  {:>7,}.    Elapsed: {:}.  Remaining: {:}'.format(step, len(py_inputs), elapsed, remaining))\n",
        "\n",
        "      # Copy the batch to the GPU.\n",
        "      b_input_ids = py_inputs[step].to(device)\n",
        "      b_input_mask = py_attn_masks[step].to(device)\n",
        "      b_labels = py_labels[step].to(device)\n",
        "    \n",
        "      # Telling the model not to compute or store gradients, saving memory and \n",
        "      # speeding up prediction\n",
        "      with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = model(b_input_ids, token_type_ids=None, \n",
        "                          attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "      logits = outputs[1] # used to be 0, should be 1 or try .logits?\n",
        "      total_loss += outputs.loss.item()\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "      # Store predictions and true labels\n",
        "      predictions.append(logits)\n",
        "      true_labels.append(label_ids)\n",
        "\n",
        "  avg_loss = total_loss / step\n",
        "\n",
        "  print('    DONE.')\n",
        "  return (predictions, true_labels, batch_ordered_sentences, batch_ordered_labels, avg_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo5ZqJR9ME_2"
      },
      "source": [
        "### Print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li-4pvYfZbru"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "def print_accuracy(predictions, true_labels, encoder, step=None, title=None):\n",
        "  # Combine the results across the batches.\n",
        "  predictions = np.concatenate(predictions, axis=0)\n",
        "  true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "  # Choose the label with the highest score as our prediction.\n",
        "  preds = np.argmax(predictions, axis=1).flatten()\n",
        "\n",
        "  # Calculate simple flat accuracy -- number correct over total number.\n",
        "  accuracy = (preds == true_labels).mean()\n",
        "  # sample k errors to find pattern\n",
        "\n",
        "  # use sklearn.metrics.f1_score here, get average f1 use macro, per class f1 pass in None to average\n",
        "  # use sklearn.metrics.confusion_matrix here, plot into wandb\n",
        "  # calculate f1 and confusion on train and val, compare train to val\n",
        "\n",
        "  #f1_per_class = f1_score(true_labels, preds, average=None)\n",
        "\n",
        "  if step is not None:\n",
        "    if title is not None:\n",
        "      wandb.log({title + ' accuracy': accuracy}, step=step)\n",
        "      f1_average = f1_score(true_labels, preds, average='macro')\n",
        "      wandb.log({title + ' f1_average': f1_average}, step=step)\n",
        "      wandb.log({title + ' confusion matrix': wandb.plot.confusion_matrix(probs=None,\n",
        "                                                                          y_true=true_labels, preds=preds,\n",
        "                                                                          class_names=encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))},\n",
        "                step=step)\n",
        "\n",
        "  # print('Accuracy: {:.3f}'.format(accuracy))\n",
        "  # print('F1: {:.3f}'.format(f1))\n",
        "  return accuracy\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPnBVnZRVisG"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhw7-nurVNGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228959fd-932b-4d43-f21b-356dc417f189"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  df = load_data(data_dir)\n",
        "  train_text, train_labels, test_text, test_labels, validation_text, validation_labels = split_train_test(df)\n",
        "  # see_train_dist(train_labels)\n",
        "  train_labels, validation_labels, test_labels, encoder = encode_labels(train_labels, validation_labels, test_labels)\n",
        "  full_input_ids, tokenizer = tokenize(train_text, bert_version)\n",
        "  # see_unsorted_data(full_input_ids)\n",
        "  train_samples = sort_data(full_input_ids, train_labels)\n",
        "  # see_sorted_data(train_samples)\n",
        "  batch_size, batch_ordered_sentences, batch_ordered_labels = select_batches(train_samples)\n",
        "  py_inputs, py_attn_masks, py_labels = add_padding(batch_ordered_sentences, batch_ordered_labels, tokenizer)\n",
        "  # see_padding(py_inputs, train_text)\n",
        "  model, device = load_model(bert_version)\n",
        "  optimizer, scheduler, epochs = get_optimizer_scheduler(model, py_inputs)\n",
        "  train_loop(model, py_inputs, py_attn_masks, py_labels, epochs, train_text, train_labels, batch_size, device, optimizer, scheduler, encoder)\n",
        "  # clear and load best model\n",
        "  # model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "  model.to(device)\n",
        "  predictions, true_labels, bos, bol, _ = eval_model(model, validation_text, validation_labels, batch_size, device)\n",
        "  val_accuracy = print_accuracy(predictions, true_labels, encoder)\n",
        "  print('Best Val Accuracy: {:.3f}'.format(val_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset contains 85 countries, test dataset contains 11 countries, and validation dataset contains 11 countries, for a total of 107 countries.\n",
            "Training dataset contains 12267 sentences, test dataset contains 969 sentences, and validation dataset contains 1059 sentences, for a total of 14295 sentences.\n",
            "Loading BERT tokenizer...\n",
            "Tokenizing 12,267 training samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "Shortest sample: 11\n",
            "Longest sample: 397\n",
            "Creating training batches of size 8\n",
            "  Selected 0 batches.\n",
            "  Selected 500 batches.\n",
            "  Selected 1,000 batches.\n",
            "  Selected 1,500 batches.\n",
            "\n",
            "  DONE - 1,534 batches.\n",
            "Config type: <class 'transformers.models.bert.configuration_bert.BertConfig'> \n",
            "\n",
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\",\n",
            "    \"9\": \"LABEL_9\",\n",
            "    \"10\": \"LABEL_10\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_10\": 10,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8,\n",
            "    \"LABEL_9\": 9\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.5.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model type: <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
            "\n",
            "Loading model to GPU...\n",
            "   GPU: Tesla T4\n",
            "   DONE.\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:19.   Remaining: 0:02:04\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:39.   Remaining: 0:01:49\n",
            "    Batch 600 of 1534.   Elapsed: 0:00:58.   Remaining: 0:01:31\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:18.   Remaining: 0:01:12\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:37.   Remaining: 0:00:52\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:57.   Remaining: 0:00:32\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:16.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:26\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:15.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:19.  Remaining: 0:00:10\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:23.  Remaining: 0:00:06\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 1.81\n",
            "   Average validation loss: 2.03\n",
            "   Training epoch took: 0:02:29\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:29\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:41.   Remaining: 0:01:57\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:00.   Remaining: 0:01:34\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:39.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:59.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:18.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:27\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:11.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:10\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 1.57\n",
            "   Average validation loss: 2.05\n",
            "   Training epoch took: 0:02:32\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:28\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:42.   Remaining: 0:01:58\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:01.   Remaining: 0:01:34\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:39.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:58.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:18.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:24\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:07.  Remaining: 0:00:21\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:11.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:10\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 1.28\n",
            "   Average validation loss: 2.27\n",
            "   Training epoch took: 0:02:32\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:23.   Remaining: 0:02:31\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:42.   Remaining: 0:01:58\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:01.   Remaining: 0:01:36\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:21.   Remaining: 0:01:14\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:40.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:59.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:18.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:26\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:11\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 1.00\n",
            "   Average validation loss: 2.51\n",
            "   Training epoch took: 0:02:32\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:23.   Remaining: 0:02:33\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:42.   Remaining: 0:01:58\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:01.   Remaining: 0:01:35\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:39.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:02:00.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:19.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:26\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:15\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:11\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.70\n",
            "   Average validation loss: 3.25\n",
            "   Training epoch took: 0:02:33\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:27\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:41.   Remaining: 0:01:58\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:01.   Remaining: 0:01:35\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:39.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:59.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:18.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:25\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:15.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:19.  Remaining: 0:00:10\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:23.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.50\n",
            "   Average validation loss: 3.58\n",
            "   Training epoch took: 0:02:33\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:28\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:42.   Remaining: 0:01:58\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:01.   Remaining: 0:01:35\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:40.   Remaining: 0:00:53\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:58.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:18.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:26\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:23\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:19\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:15\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:11\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.34\n",
            "   Average validation loss: 4.18\n",
            "   Training epoch took: 0:02:32\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:25\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:41.   Remaining: 0:01:56\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:00.   Remaining: 0:01:33\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:20.   Remaining: 0:01:13\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:38.   Remaining: 0:00:52\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:57.   Remaining: 0:00:33\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:16.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:25\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:11\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.24\n",
            "   Average validation loss: 5.12\n",
            "   Training epoch took: 0:02:29\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:22.   Remaining: 0:02:27\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:41.   Remaining: 0:01:56\n",
            "    Batch 600 of 1534.   Elapsed: 0:01:00.   Remaining: 0:01:33\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:19.   Remaining: 0:01:12\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:37.   Remaining: 0:00:52\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:56.   Remaining: 0:00:32\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:16.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:26\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:08.  Remaining: 0:00:22\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:11\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.15\n",
            "   Average validation loss: 5.53\n",
            "   Training epoch took: 0:02:29\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Training on 1534 batches...\n",
            "    Batch 200 of 1534.   Elapsed: 0:00:21.   Remaining: 0:02:23\n",
            "    Batch 400 of 1534.   Elapsed: 0:00:40.   Remaining: 0:01:54\n",
            "    Batch 600 of 1534.   Elapsed: 0:00:59.   Remaining: 0:01:32\n",
            "    Batch 800 of 1534.   Elapsed: 0:01:18.   Remaining: 0:01:12\n",
            "    Batch 1000 of 1534.   Elapsed: 0:01:37.   Remaining: 0:00:52\n",
            "    Batch 1200 of 1534.   Elapsed: 0:01:55.   Remaining: 0:00:32\n",
            "    Batch 1400 of 1534.   Elapsed: 0:02:14.   Remaining: 0:00:13\n",
            "Creating Smart Batches from 12,267 examples with batch size 8...\n",
            "\n",
            "Tokenizing 12,267 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "  Tokenized 2,000 samples.\n",
            "  Tokenized 3,000 samples.\n",
            "  Tokenized 4,000 samples.\n",
            "  Tokenized 5,000 samples.\n",
            "  Tokenized 6,000 samples.\n",
            "  Tokenized 7,000 samples.\n",
            "  Tokenized 8,000 samples.\n",
            "  Tokenized 9,000 samples.\n",
            "  Tokenized 10,000 samples.\n",
            "  Tokenized 11,000 samples.\n",
            "  Tokenized 12,000 samples.\n",
            "DONE.\n",
            "    12,267 samples\n",
            "\n",
            "    12,267 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 1,000 batches.\n",
            "\n",
            "  DONE - Selected 1,534 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 12,267 test sentences...\n",
            "  Batch     200  of    1,534.    Elapsed: 0:00:04.  Remaining: 0:00:25\n",
            "  Batch     400  of    1,534.    Elapsed: 0:00:07.  Remaining: 0:00:21\n",
            "  Batch     600  of    1,534.    Elapsed: 0:00:12.  Remaining: 0:00:18\n",
            "  Batch     800  of    1,534.    Elapsed: 0:00:16.  Remaining: 0:00:14\n",
            "  Batch   1,000  of    1,534.    Elapsed: 0:00:20.  Remaining: 0:00:10\n",
            "  Batch   1,200  of    1,534.    Elapsed: 0:00:24.  Remaining: 0:00:07\n",
            "  Batch   1,400  of    1,534.    Elapsed: 0:00:28.  Remaining: 0:00:03\n",
            "    DONE.\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:02\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      70  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "\n",
            "   Average training loss: 0.09\n",
            "   Average validation loss: 5.72\n",
            "   Training epoch took: 0:02:28\n",
            "Creating Smart Batches from 1,059 examples with batch size 8...\n",
            "\n",
            "Tokenizing 1,059 samples...\n",
            "  Tokenized 0 samples.\n",
            "  Tokenized 100 samples.\n",
            "  Tokenized 200 samples.\n",
            "  Tokenized 300 samples.\n",
            "  Tokenized 400 samples.\n",
            "  Tokenized 500 samples.\n",
            "  Tokenized 600 samples.\n",
            "  Tokenized 700 samples.\n",
            "  Tokenized 800 samples.\n",
            "  Tokenized 900 samples.\n",
            "  Tokenized 1,000 samples.\n",
            "DONE.\n",
            "     1,059 samples\n",
            "\n",
            "     1,059 samples after sorting\n",
            "\n",
            "Creating batches of size 8...\n",
            "  Selected 100 batches.\n",
            "\n",
            "  DONE - Selected 133 batches.\n",
            "\n",
            "Padding out sequences within each batch...\n",
            "  DONE.\n",
            "Predicting labels for 1,059 test sentences...\n",
            "  Batch      10  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      20  of      133.    Elapsed: 0:00:00.  Remaining: 0:00:03\n",
            "  Batch      30  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      40  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      50  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      60  of      133.    Elapsed: 0:00:01.  Remaining: 0:00:02\n",
            "  Batch      70  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      80  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch      90  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     100  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:01\n",
            "  Batch     110  of      133.    Elapsed: 0:00:02.  Remaining: 0:00:00\n",
            "  Batch     120  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "  Batch     130  of      133.    Elapsed: 0:00:03.  Remaining: 0:00:00\n",
            "    DONE.\n",
            "Best Val Accuracy: 0.274\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}